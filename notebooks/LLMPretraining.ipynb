{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2hetj6BWaux3",
        "outputId": "0847683a-e19d-4a80-c798-34ce48608349"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.6 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━\u001b[0m \u001b[32m1.2/1.6 MB\u001b[0m \u001b[31m38.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m30.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m78.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.7/2.7 MB\u001b[0m \u001b[31m102.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for sorted_nearest (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Installed Python deps and downloaded gdc-client.\n"
          ]
        }
      ],
      "source": [
        "!pip install pandas numpy pyarrow requests tqdm pyranges --quiet\n",
        "\n",
        "# Download GDC client\n",
        "!wget https://gdc.cancer.gov/files/public/file/gdc-client_v1.6.1_Ubuntu_x64.zip -O gdc.zip -q\n",
        "!unzip -o gdc.zip > /dev/null\n",
        "!chmod +x gdc-client\n",
        "\n",
        "print(\"Installed Python deps and downloaded gdc-client.\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "print(\" Google Drive mounted at /content/drive\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S29wot_IavoE",
        "outputId": "369a409a-c308-414f-9ca2-7e0971218669"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            " Google Drive mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "import os\n",
        "import pandas as pd\n",
        "from transformers import (\n",
        "    BertConfig,\n",
        "    BertForMaskedLM,\n",
        "    DataCollatorForLanguageModeling,\n",
        "    Trainer,\n",
        "    TrainingArguments,\n",
        "    TrainerCallback\n",
        ")\n",
        "\n",
        "# Main project root on Google Drive\n",
        "PROJECT_ROOT = Path(\"/content/drive/MyDrive/bdh_challenge_2025_data\")\n",
        "PROJECT_ROOT.mkdir(exist_ok=True)\n",
        "\n",
        "# Data directories\n",
        "\n",
        "# Generic data directory\n",
        "DATA_DIR = PROJECT_ROOT / \"data\"\n",
        "DATA_DIR.mkdir(exist_ok=True)\n",
        "\n",
        "# Directory where raw TCGA STAR count files will be stored\n",
        "RNA_DIR = PROJECT_ROOT / \"tcga_rna\"\n",
        "RNA_DIR.mkdir(exist_ok=True)\n",
        "\n",
        "# Directory where processed matrices, tokenized data, embeddings\n",
        "PROCESSED_DIR = PROJECT_ROOT / \"processed\"\n",
        "PROCESSED_DIR.mkdir(exist_ok=True)\n",
        "\n",
        "print(\"Project root :\", PROJECT_ROOT)\n",
        "print(\"DATA_DIR     :\", DATA_DIR)\n",
        "print(\"RNA_DIR      :\", RNA_DIR)\n",
        "print(\"PROCESSED_DIR:\", PROCESSED_DIR)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TVOVdjGdb9K3",
        "outputId": "30236d4c-4fa5-4e39-dcb4-77e5f80c7655"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Project root : /content/drive/MyDrive/bdh_challenge_2025_data\n",
            "DATA_DIR     : /content/drive/MyDrive/bdh_challenge_2025_data/data\n",
            "RNA_DIR      : /content/drive/MyDrive/bdh_challenge_2025_data/tcga_rna\n",
            "PROCESSED_DIR: /content/drive/MyDrive/bdh_challenge_2025_data/processed\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ALIGNED_PATH = PROCESSED_DIR / \"tcga_tpm_like_log10_bulkrnabert_aligned.parquet\"\n",
        "expr_aligned = pd.read_parquet(ALIGNED_PATH)"
      ],
      "metadata": {
        "id": "mWU7nms7mZpa"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "expr_aligned.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        },
        "id": "BXyOhWIU1PuG",
        "outputId": "b10b86f2-d0e8-4cad-c421-88c77a9b0776"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "gene_id       ENSG00000000003  ENSG00000000005  ENSG00000000419  \\\n",
              "sample_id                                                         \n",
              "TCGA-P5-A5EY         1.681527         0.063817         1.141174   \n",
              "TCGA-AR-A1AW         1.605939         0.088174         1.567791   \n",
              "TCGA-E9-A1N5         1.848499         0.367775         1.398195   \n",
              "TCGA-97-7941         2.254924         0.079794         1.397967   \n",
              "TCGA-93-7347         1.395531         0.035987         1.261120   \n",
              "\n",
              "gene_id       ENSG00000000457  ENSG00000000460  ENSG00000000938  \\\n",
              "sample_id                                                         \n",
              "TCGA-P5-A5EY         0.877901         0.408766         0.778577   \n",
              "TCGA-AR-A1AW         1.419358         1.132621         1.467036   \n",
              "TCGA-E9-A1N5         1.432945         0.981104         1.023865   \n",
              "TCGA-97-7941         1.311801         0.662706         1.375285   \n",
              "TCGA-93-7347         1.125617         0.640414         1.382043   \n",
              "\n",
              "gene_id       ENSG00000000971  ENSG00000001036  ENSG00000001084  \\\n",
              "sample_id                                                         \n",
              "TCGA-P5-A5EY         0.989684         1.165323         1.534292   \n",
              "TCGA-AR-A1AW         2.035625         1.477680         1.519647   \n",
              "TCGA-E9-A1N5         1.629135         1.540959         1.538390   \n",
              "TCGA-97-7941         1.929090         1.767373         1.938894   \n",
              "TCGA-93-7347         1.897127         1.634034         1.067374   \n",
              "\n",
              "gene_id       ENSG00000001167  ...  ENSG00000284519  ENSG00000284532  \\\n",
              "sample_id                      ...                                     \n",
              "TCGA-P5-A5EY         1.396228  ...              0.0              0.0   \n",
              "TCGA-AR-A1AW         1.800230  ...              0.0              0.0   \n",
              "TCGA-E9-A1N5         1.612078  ...              0.0              0.0   \n",
              "TCGA-97-7941         1.489701  ...              0.0              0.0   \n",
              "TCGA-93-7347         1.466133  ...              0.0              0.0   \n",
              "\n",
              "gene_id       ENSG00000284535  ENSG00000284543  ENSG00000284557  \\\n",
              "sample_id                                                         \n",
              "TCGA-P5-A5EY              0.0         0.008509              0.0   \n",
              "TCGA-AR-A1AW              0.0         0.254434              0.0   \n",
              "TCGA-E9-A1N5              0.0         0.189368              0.0   \n",
              "TCGA-97-7941              0.0         0.393385              0.0   \n",
              "TCGA-93-7347              0.0         0.241292              0.0   \n",
              "\n",
              "gene_id       ENSG00000284564  ENSG00000284574  ENSG00000284587  \\\n",
              "sample_id                                                         \n",
              "TCGA-P5-A5EY              0.0              0.0              0.0   \n",
              "TCGA-AR-A1AW              0.0              0.0              0.0   \n",
              "TCGA-E9-A1N5              0.0              0.0              0.0   \n",
              "TCGA-97-7941              0.0              0.0              0.0   \n",
              "TCGA-93-7347              0.0              0.0              0.0   \n",
              "\n",
              "gene_id       ENSG00000284595  ENSG00000284596  \n",
              "sample_id                                       \n",
              "TCGA-P5-A5EY              0.0              0.0  \n",
              "TCGA-AR-A1AW              0.0              0.0  \n",
              "TCGA-E9-A1N5              0.0              0.0  \n",
              "TCGA-97-7941              0.0              0.0  \n",
              "TCGA-93-7347              0.0              0.0  \n",
              "\n",
              "[5 rows x 19062 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-08667023-d045-43a2-9e7f-c007388bc7b5\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th>gene_id</th>\n",
              "      <th>ENSG00000000003</th>\n",
              "      <th>ENSG00000000005</th>\n",
              "      <th>ENSG00000000419</th>\n",
              "      <th>ENSG00000000457</th>\n",
              "      <th>ENSG00000000460</th>\n",
              "      <th>ENSG00000000938</th>\n",
              "      <th>ENSG00000000971</th>\n",
              "      <th>ENSG00000001036</th>\n",
              "      <th>ENSG00000001084</th>\n",
              "      <th>ENSG00000001167</th>\n",
              "      <th>...</th>\n",
              "      <th>ENSG00000284519</th>\n",
              "      <th>ENSG00000284532</th>\n",
              "      <th>ENSG00000284535</th>\n",
              "      <th>ENSG00000284543</th>\n",
              "      <th>ENSG00000284557</th>\n",
              "      <th>ENSG00000284564</th>\n",
              "      <th>ENSG00000284574</th>\n",
              "      <th>ENSG00000284587</th>\n",
              "      <th>ENSG00000284595</th>\n",
              "      <th>ENSG00000284596</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>sample_id</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>TCGA-P5-A5EY</th>\n",
              "      <td>1.681527</td>\n",
              "      <td>0.063817</td>\n",
              "      <td>1.141174</td>\n",
              "      <td>0.877901</td>\n",
              "      <td>0.408766</td>\n",
              "      <td>0.778577</td>\n",
              "      <td>0.989684</td>\n",
              "      <td>1.165323</td>\n",
              "      <td>1.534292</td>\n",
              "      <td>1.396228</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.008509</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>TCGA-AR-A1AW</th>\n",
              "      <td>1.605939</td>\n",
              "      <td>0.088174</td>\n",
              "      <td>1.567791</td>\n",
              "      <td>1.419358</td>\n",
              "      <td>1.132621</td>\n",
              "      <td>1.467036</td>\n",
              "      <td>2.035625</td>\n",
              "      <td>1.477680</td>\n",
              "      <td>1.519647</td>\n",
              "      <td>1.800230</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.254434</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>TCGA-E9-A1N5</th>\n",
              "      <td>1.848499</td>\n",
              "      <td>0.367775</td>\n",
              "      <td>1.398195</td>\n",
              "      <td>1.432945</td>\n",
              "      <td>0.981104</td>\n",
              "      <td>1.023865</td>\n",
              "      <td>1.629135</td>\n",
              "      <td>1.540959</td>\n",
              "      <td>1.538390</td>\n",
              "      <td>1.612078</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.189368</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>TCGA-97-7941</th>\n",
              "      <td>2.254924</td>\n",
              "      <td>0.079794</td>\n",
              "      <td>1.397967</td>\n",
              "      <td>1.311801</td>\n",
              "      <td>0.662706</td>\n",
              "      <td>1.375285</td>\n",
              "      <td>1.929090</td>\n",
              "      <td>1.767373</td>\n",
              "      <td>1.938894</td>\n",
              "      <td>1.489701</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.393385</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>TCGA-93-7347</th>\n",
              "      <td>1.395531</td>\n",
              "      <td>0.035987</td>\n",
              "      <td>1.261120</td>\n",
              "      <td>1.125617</td>\n",
              "      <td>0.640414</td>\n",
              "      <td>1.382043</td>\n",
              "      <td>1.897127</td>\n",
              "      <td>1.634034</td>\n",
              "      <td>1.067374</td>\n",
              "      <td>1.466133</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.241292</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 19062 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-08667023-d045-43a2-9e7f-c007388bc7b5')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-08667023-d045-43a2-9e7f-c007388bc7b5 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-08667023-d045-43a2-9e7f-c007388bc7b5');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-b4868d54-7814-4aa6-8a02-2622e5b17cec\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-b4868d54-7814-4aa6-8a02-2622e5b17cec')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-b4868d54-7814-4aa6-8a02-2622e5b17cec button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "expr_aligned"
            }
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import BertConfig, BertForMaskedLM, PreTrainedModel, BertModel\n",
        "\n",
        "\n",
        "CONFIG = {\n",
        "    \"N_genes\": 19062,\n",
        "    \"n_embedding\": 256,\n",
        "    \"n_layers\": 4,\n",
        "    \"n_heads\": 8,\n",
        "    \"n_bins\": 64,\n",
        "    \"vocab_size\": 70,\n",
        "    \"mask_prob\": 0.15,\n",
        "    \"batch_size_eff\": 3e6,\n",
        "    \"batch_size\": 16,\n",
        "    \"hv_genes_count\": 12403,\n",
        "    \"grad_accum\": 4,\n",
        "    \"lr\": 1e-4,\n",
        "    \"max_steps\": 10000,\n",
        "}"
      ],
      "metadata": {
        "id": "IAM1JO881Pq8"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class RNAPreprocessor:\n",
        "    def __init__(self, num_bins=64):\n",
        "        self.num_bins = num_bins\n",
        "        self.global_max = None\n",
        "        self.bin_edges = None\n",
        "\n",
        "    def fit_transform(self, data):\n",
        "\n",
        "        transformed = data # data is already log10(1+x) transformed\n",
        "\n",
        "        # Max Normalization\n",
        "        self.global_max = np.max(transformed)\n",
        "        normalized = transformed / self.global_max\n",
        "\n",
        "        # Binning\n",
        "        self.bin_edges = np.linspace(0, 1, self.num_bins + 1)\n",
        "        binned = np.digitize(normalized, self.bin_edges) - 1\n",
        "        binned = np.clip(binned, 0, self.num_bins - 1)\n",
        "\n",
        "        # Shift +5 for special tokens (0-4 reserved)\n",
        "        return binned + 5\n",
        "\n",
        "class BulkRNADataset(Dataset):\n",
        "    def __init__(self, tokenized_data, survival_time=None, event_indicator=None):\n",
        "        self.data = torch.tensor(tokenized_data, dtype=torch.long)\n",
        "        self.survival_time = torch.tensor(survival_time, dtype=torch.float) if survival_time is not None else None\n",
        "        self.event = torch.tensor(event_indicator, dtype=torch.float) if event_indicator is not None else None\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = {\"input_ids\": self.data[idx]}\n",
        "        if self.survival_time is not None:\n",
        "            item[\"time\"] = self.survival_time[idx]\n",
        "            item[\"event\"] = self.event[idx]\n",
        "        return item"
      ],
      "metadata": {
        "id": "Cxm3DugB1PoC"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class MockTokenizer:\n",
        "    \"\"\"\n",
        "    A dummy tokenizer that satisfies all DataCollator requirements:\n",
        "    1. Provides special token attributes.\n",
        "    2. Implements padding (stacking tensors).\n",
        "    3. Identifies special tokens so they aren't masked.\n",
        "    \"\"\"\n",
        "    def __init__(self, vocab_size, mask_token_id=4, pad_token_id=0):\n",
        "        self.vocab_size = vocab_size\n",
        "        self.mask_token_id = mask_token_id\n",
        "        self.pad_token_id = pad_token_id\n",
        "\n",
        "        # Attributes required by DataCollator checks\n",
        "        self.mask_token = \"[MASK]\"\n",
        "        self.pad_token = \"[PAD]\"\n",
        "\n",
        "        # Prevents truncation warnings\n",
        "        self.model_max_length = 100_000\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.vocab_size\n",
        "\n",
        "    def pad(self, encoded_inputs, return_tensors=\"pt\", **kwargs):\n",
        "        \"\"\"\n",
        "        Called by DataCollator to stack a list of samples into a batch.\n",
        "        Since our gene expression data is fixed-length (N_genes), we just stack them.\n",
        "        \"\"\"\n",
        "        import torch\n",
        "\n",
        "        # encoded_inputs is a list of dicts: [{'input_ids': tensor}, {'input_ids': tensor}]\n",
        "        batch = {}\n",
        "\n",
        "        # We take the first sample to know the keys (usually just 'input_ids')\n",
        "        for key in encoded_inputs[0].keys():\n",
        "            # Extract values for this key from all samples\n",
        "            values = [d[key] for d in encoded_inputs]\n",
        "\n",
        "            # If they are already tensors, stack them. If lists, convert to tensor.\n",
        "            if isinstance(values[0], torch.Tensor):\n",
        "                batch[key] = torch.stack(values)\n",
        "            else:\n",
        "                batch[key] = torch.tensor(values, dtype=torch.long)\n",
        "\n",
        "        return batch\n",
        "\n",
        "    def get_special_tokens_mask(self, token_ids_0, token_ids_1=None, already_has_special_tokens=False):\n",
        "        \"\"\"\n",
        "        Called by DataCollator to decide what NOT to mask.\n",
        "        We treat IDs 0 to 4 as special (PAD, UNK, CLS, SEP, MASK).\n",
        "        Returns: list of 0s (can mask) and 1s (cannot mask).\n",
        "        \"\"\"\n",
        "        # token_ids_0 is a list of integers\n",
        "        return [1 if token < 5 else 0 for token in token_ids_0]\n",
        "\n",
        "    def convert_tokens_to_ids(self, token):\n",
        "        return self.mask_token_id if token == self.mask_token else 0\n",
        "\n",
        "    def save_pretrained(self, save_directory):\n",
        "        pass\n",
        "\n",
        "def train_pretraining_model(train_dataset, eval_dataset):\n",
        "    \"\"\"\n",
        "    Sets up the BERT MLM training loop using Hugging Face Trainer.\n",
        "    Matches the \"Pre-training\" section of the paper.\n",
        "    \"\"\"\n",
        "    tokenizer = MockTokenizer(\n",
        "        vocab_size=CONFIG['vocab_size'],\n",
        "        mask_token_id=4,\n",
        "        pad_token_id=0\n",
        "    )\n",
        "    # 1. Architecture\n",
        "    config = BertConfig(\n",
        "        vocab_size=CONFIG['vocab_size'],\n",
        "        hidden_size=CONFIG['n_embedding'],\n",
        "        num_hidden_layers=CONFIG['n_layers'],\n",
        "        num_attention_heads=CONFIG['n_heads'],\n",
        "        max_position_embeddings=CONFIG['N_genes'] + 512, # Buffer\n",
        "        pad_token_id=0,\n",
        "        mask_token_id=4\n",
        "    )\n",
        "\n",
        "    model = BertForMaskedLM(config)\n",
        "    print(f\"Model Parameters: {model.num_parameters()}\")\n",
        "\n",
        "    # 2. Collator (Handles the 15% masking automatically)\n",
        "    # Paper: \"decided not to consider only non-zero... but all of them\"\n",
        "    data_collator = DataCollatorForLanguageModeling(\n",
        "        tokenizer=tokenizer, # Not needed for custom IDs\n",
        "        mlm=True,\n",
        "        mlm_probability=CONFIG['mask_prob']\n",
        "    )\n",
        "\n",
        "    # 3. Training Arguments\n",
        "    # Paper uses huge batch size (3e6 tokens). We simulate via accumulation.\n",
        "    training_args = TrainingArguments(\n",
        "        output_dir=\"./results_pretrain\",\n",
        "        overwrite_output_dir=True,\n",
        "        num_train_epochs=15,\n",
        "        per_device_train_batch_size=CONFIG['batch_size'],\n",
        "        gradient_accumulation_steps=CONFIG['grad_accum'],\n",
        "        learning_rate=CONFIG['lr'],\n",
        "        logging_steps=50,\n",
        "        save_steps=500,\n",
        "        eval_strategy=\"steps\",\n",
        "        eval_steps=100,\n",
        "        fp16=torch.cuda.is_available(), # Use mixed precision if GPU available\n",
        "        report_to=\"none\"\n",
        "    )\n",
        "\n",
        "    # 4. Metrics (Reconstruction Accuracy)\n",
        "    def compute_metrics(eval_pred):\n",
        "        logits, labels = eval_pred\n",
        "        predictions = np.argmax(logits, axis=-1)\n",
        "        # Masked tokens are those where label != -100\n",
        "        mask = labels != -100\n",
        "        accuracy = (predictions[mask] == labels[mask]).mean()\n",
        "        return {\"accuracy\": accuracy}\n",
        "\n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        train_dataset=train_dataset,\n",
        "        eval_dataset=eval_dataset,\n",
        "        data_collator=data_collator,\n",
        "        compute_metrics=compute_metrics\n",
        "    )\n",
        "\n",
        "    return trainer"
      ],
      "metadata": {
        "id": "HJPUBT_W1Pk6"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n",
        "print(\"\\nPreprocessing Data...\")\n",
        "preprocessor = RNAPreprocessor(num_bins=CONFIG['n_bins'])\n",
        "tokenized_data = preprocessor.fit_transform(expr_aligned)\n",
        "\n",
        "# Split Train/Test (95/5 split as per paper)\n",
        "train_data, test_data = train_test_split(tokenized_data, test_size=0.05)\n",
        "\n",
        "train_dataset = BulkRNADataset(train_data)\n",
        "eval_dataset = BulkRNADataset(test_data)\n",
        "\n",
        "print(f\"Train size: {len(train_dataset)}, Eval size: {len(eval_dataset)}\")\n",
        "\n",
        "# --- PART C: Pre-train (MLM) ---\n",
        "print(\"\\nStarting Pre-training...\")\n",
        "trainer = train_pretraining_model(train_dataset, eval_dataset)\n",
        "\n",
        "# Run Training\n",
        "trainer.train() # Uncomment to execute\n",
        "print(\"Training setup complete. Call trainer.train() to start.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 454
        },
        "id": "5Qky2XQU1PiU",
        "outputId": "12ea6b30-5d20-46d2-9491-6a44801dd93e"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Preprocessing Data...\n",
            "Train size: 3215, Eval size: 170\n",
            "\n",
            "Starting Pre-training...\n",
            "Model Parameters: 12457798\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='765' max='765' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [765/765 54:23, Epoch 15/15]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>2.481200</td>\n",
              "      <td>2.243885</td>\n",
              "      <td>0.356784</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>2.111100</td>\n",
              "      <td>2.038057</td>\n",
              "      <td>0.370813</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>300</td>\n",
              "      <td>2.046400</td>\n",
              "      <td>1.996385</td>\n",
              "      <td>0.373528</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>400</td>\n",
              "      <td>2.023200</td>\n",
              "      <td>1.977154</td>\n",
              "      <td>0.375163</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>2.007300</td>\n",
              "      <td>1.969604</td>\n",
              "      <td>0.374517</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>600</td>\n",
              "      <td>2.001400</td>\n",
              "      <td>1.958970</td>\n",
              "      <td>0.376751</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>700</td>\n",
              "      <td>1.997500</td>\n",
              "      <td>1.959643</td>\n",
              "      <td>0.376523</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "We strongly recommend passing in an `attention_mask` since your input_ids may be padded. See https://huggingface.co/docs/transformers/troubleshooting#incorrect-output-when-padding-tokens-arent-masked.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training setup complete. Call trainer.train() to start.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertModel, BertConfig\n",
        "\n",
        "\n",
        "checkpoint_path = \"./results_pretrain/checkpoint-765\"\n",
        "\n",
        "\n",
        "model_mlm = BertForMaskedLM.from_pretrained(checkpoint_path)\n",
        "\n",
        "\n",
        "bert_encoder = BertModel.from_pretrained(checkpoint_path)\n",
        "\n",
        "print(\"Encoder loaded successfully. Ready for fine-tuning.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XuursgVB1PfM",
        "outputId": "ffc9e38c-e2b8-4770-8149-66c80adf0332"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertModel were not initialized from the model checkpoint at ./results_pretrain/checkpoint-765 and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Encoder loaded successfully. Ready for fine-tuning.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "bert_encoder"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uAlFCNzfaNJ3",
        "outputId": "a7e438b8-3875-4cd0-f20a-3841058d305c"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BertModel(\n",
              "  (embeddings): BertEmbeddings(\n",
              "    (word_embeddings): Embedding(70, 256, padding_idx=0)\n",
              "    (position_embeddings): Embedding(19574, 256)\n",
              "    (token_type_embeddings): Embedding(2, 256)\n",
              "    (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
              "    (dropout): Dropout(p=0.1, inplace=False)\n",
              "  )\n",
              "  (encoder): BertEncoder(\n",
              "    (layer): ModuleList(\n",
              "      (0-3): 4 x BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSdpaSelfAttention(\n",
              "            (query): Linear(in_features=256, out_features=256, bias=True)\n",
              "            (key): Linear(in_features=256, out_features=256, bias=True)\n",
              "            (value): Linear(in_features=256, out_features=256, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=256, out_features=256, bias=True)\n",
              "            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=256, out_features=3072, bias=True)\n",
              "          (intermediate_act_fn): GELUActivation()\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=256, bias=True)\n",
              "          (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (pooler): BertPooler(\n",
              "    (dense): Linear(in_features=256, out_features=256, bias=True)\n",
              "    (activation): Tanh()\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Fi68sD6a1PZP"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xq9sVaBC1PWV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "source_folder = \"./results_pretrain/checkpoint-765\"\n",
        "\n",
        "destination_folder = PROJECT_ROOT / \"BulkRNABert_Models\"\n",
        "import subprocess\n",
        "\n",
        "\n",
        "os.makedirs(destination_folder, exist_ok=True)\n",
        "\n",
        "print(f\"Copying files to {destination_folder}...\")\n",
        "\n",
        "!cp -r -v \"$source_folder\" \"$destination_folder\"\n",
        "\n",
        "print(\"Copy complete.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2bshTjbO1PTg",
        "outputId": "ca0d2d37-bb4f-44d7-c7ad-d4673aa6a437"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Copying files to /content/drive/MyDrive/bdh_challenge_2025_data/BulkRNABert_Models...\n",
            "'./results_pretrain/checkpoint-765/config.json' -> '/content/drive/MyDrive/bdh_challenge_2025_data/BulkRNABert_Models/checkpoint-765/config.json'\n",
            "'./results_pretrain/checkpoint-765/model.safetensors' -> '/content/drive/MyDrive/bdh_challenge_2025_data/BulkRNABert_Models/checkpoint-765/model.safetensors'\n",
            "'./results_pretrain/checkpoint-765/training_args.bin' -> '/content/drive/MyDrive/bdh_challenge_2025_data/BulkRNABert_Models/checkpoint-765/training_args.bin'\n",
            "'./results_pretrain/checkpoint-765/optimizer.pt' -> '/content/drive/MyDrive/bdh_challenge_2025_data/BulkRNABert_Models/checkpoint-765/optimizer.pt'\n",
            "'./results_pretrain/checkpoint-765/scheduler.pt' -> '/content/drive/MyDrive/bdh_challenge_2025_data/BulkRNABert_Models/checkpoint-765/scheduler.pt'\n",
            "'./results_pretrain/checkpoint-765/scaler.pt' -> '/content/drive/MyDrive/bdh_challenge_2025_data/BulkRNABert_Models/checkpoint-765/scaler.pt'\n",
            "'./results_pretrain/checkpoint-765/rng_state.pth' -> '/content/drive/MyDrive/bdh_challenge_2025_data/BulkRNABert_Models/checkpoint-765/rng_state.pth'\n",
            "'./results_pretrain/checkpoint-765/trainer_state.json' -> '/content/drive/MyDrive/bdh_challenge_2025_data/BulkRNABert_Models/checkpoint-765/trainer_state.json'\n",
            "Copy complete.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "erZNPZge1PQm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KLftRpJWcZMk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xevhVn3F1PNN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "a39j4G0K1PIy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "JWbqPshP1O_q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Hc-0HbyD1OwE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Md5Z6htomgto"
      },
      "execution_count": 27,
      "outputs": []
    }
  ]
}